{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98bb8278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/sai/anaconda3/lib/python3.11/site-packages (1.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28d848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv, os\n",
    "import openai\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "openai.api_key = os.getenv('GPT4_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ffdd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection\n",
    "def get_completion(prompt, model=\"gpt-4-1106-preview\"):\n",
    "    messages = [{\"role\": \"user\", \"content\":prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model = model,\n",
    "        messages = messages,\n",
    "        temperature = 0  # Degree of random ness model output\n",
    "    )\n",
    "    \n",
    "#     response = client.chat.completions.create(\n",
    "#     messages=messages,\n",
    "#     model=model,\n",
    "#    temperature = 0 \n",
    "\n",
    "   \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92f4e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The error message you\\'re encountering indicates that there\\'s an issue with the `aten.convolution.default` operation when using the `inductor` backend, which seems to be part of the TorchDynamo project. TorchDynamo is a Python-level JIT compiler designed to optimize PyTorch models by dynamically compiling Python code to more efficient machine code.\\n\\nThe specific error `Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)` suggests that there\\'s a mismatch between the expected device (likely GPU, since it mentions \\'cuda\\') and the actual device of the tensor (which the error suspects might be a CPU tensor).\\n\\nHere are some steps you can take to troubleshoot and potentially solve the issue:\\n\\n1. **Check Tensor Device**: Ensure that all tensors involved in the convolution operation are on the correct device (GPU if you\\'re using CUDA). You can check the device of a tensor using `tensor.device`.\\n\\n2. **Move Tensors to GPU**: If any tensors are on the CPU, move them to the GPU using the `.to(\\'cuda\\')` method or `.cuda()` method before performing operations that require GPU computation.\\n\\n3. **Update Libraries**: Make sure that you have the latest versions of PyTorch and any other relevant libraries. Sometimes bugs are fixed in newer releases.\\n\\n4. **Verbose Logging**: As suggested by the error message, enable verbose logging to get more detailed information about the error. Set the environment variables `TORCH_LOGS=\"+dynamo\"` and `TORCHDYNAMO_VERBOSE=1` before running your script.\\n\\n5. **Fallback to Eager Execution**: If you need a quick workaround and are willing to potentially sacrifice some performance, you can suppress the error and fall back to eager execution by setting the `suppress_errors` flag in the `torch._dynamo` module:\\n\\n```python\\nimport torch._dynamo\\ntorch._dynamo.config.suppress_errors = True\\n```\\n\\n6. **Review Model and Input Data**: Double-check your model definition and input data to ensure they are correctly formatted and compatible with the operations you\\'re trying to perform.\\n\\n7. **Check for Known Issues**: Look at the TorchDynamo GitHub repository or forums for known issues or similar bug reports. It\\'s possible that others have encountered the same problem and a solution or workaround has been posted.\\n\\n8. **Report the Issue**: If none of the above steps resolve the issue, consider reporting it to the maintainers of TorchDynamo. Provide them with a minimal reproducible example, if possible, so they can better understand and address the problem.\\n\\nRemember to handle any sensitive information appropriately if you\\'re sharing code or error logs publicly.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= f\"\"\"\n",
    "Error generating image: backend='inductor' raised:\\nLoweringException: ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)\\n  target: aten.convolution.default\\n  args[0]: TensorBox(StorageBox(\\n    InputBuffer(name='arg1680_1', layout=FixedLayout('cuda', torch.bfloat16, size=[2, 4, 128, 128], stride=[65536, 16384, 128, 1]))\\n  ))\\n  args[1]: TensorBox(StorageBox(\\n    InputBuffer(name='arg184_1', layout=FixedLayout('cuda', torch.bfloat16, size=[320, 4, 3, 3], stride=[36, 1, 12, 4]))\\n  ))\\n  args[2]: TensorBox(StorageBox(\\n    InputBuffer(name='arg185_1', layout=FixedLayout('cuda', torch.bfloat16, size=[320], stride=[1]))\\n  ))\\n  args[3]: [1, 1]\\n  args[4]: [1, 1]\\n  args[5]: [1, 1]\\n  args[6]: False\\n  args[7]: [0, 0]\\n  args[8]: 1\\n\\nSet TORCH_LOGS=\\\"+dynamo\\\" and TORCHDYNAMO_VERBOSE=1 for more information\\n\\n\\nYou can suppress this exception and fall back to eager by setting:\\n    import torch._dynamo\\n    torch._dynamo.config.suppress_errors = True\\n\n",
    "\"\"\"\n",
    "\n",
    "prompt=f\"\"\"\n",
    "You are an Expert and Deep understaing deep learning libraies in python. \\\n",
    "Can you solve the error which I am facing while running the model for low latency inference.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9361b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To optimize the usage of your 8 A100 GPUs for low latency inference and ensure that they are being utilized to their full potential, you can follow these steps:\\n\\n1. **Load Balancing Configuration**: Ensure that your NGINX load balancer is configured correctly to distribute the incoming requests evenly across all available ports (9000 to 9007). You should use a load balancing method that suits your use case, such as round-robin, least connections, or IP hash.\\n\\n2. **Monitoring GPU Utilization**: Use tools like `nvidia-smi`, `nvtop`, or `gpustat` to monitor the real-time utilization of each GPU. This will help you understand if the load is being distributed evenly and if there are any bottlenecks.\\n\\n3. **Model Serving Framework**: Consider using a model serving framework optimized for GPUs, such as NVIDIA Triton Inference Server or TensorFlow Serving with GPU support. These frameworks are designed to handle concurrent model execution on GPUs efficiently.\\n\\n4. **Batching**: If your model supports it, implement request batching. Batching combines multiple inference requests into a single batch to be processed by the GPU, which can significantly improve throughput and GPU utilization.\\n\\n5. **Concurrency and Streams**: Ensure that your inference pipeline is set up to handle concurrent requests. You can use CUDA streams to run multiple inference tasks concurrently on the same GPU.\\n\\n6. **Optimize Model**: Optimize your model using techniques like quantization, layer fusion, and pruning to reduce the computational load and memory footprint, which can lead to faster inference times.\\n\\n7. **GPU Memory Management**: Make sure that your pipeline is efficiently managing GPU memory. Avoid unnecessary data transfers between the CPU and GPU, and ensure that memory is being freed when it's no longer needed.\\n\\n8. **Profiling**: Use profiling tools like NVIDIA's Nsight Systems or Nsight Compute to identify performance bottlenecks in your pipeline. These tools can provide insights into kernel execution, memory transfers, and other critical performance metrics.\\n\\n9. **Infrastructure**: Verify that your server's CPU, memory, and network are not becoming bottlenecks. If the CPU is overloaded or if there's not enough network bandwidth, it can lead to underutilization of the GPUs.\\n\\n10. **Error Handling**: Make sure that your pipeline has proper error handling to avoid any single point of failure that could disrupt the load distribution.\\n\\n11. **Software Versions**: Ensure that you are using the latest versions of CUDA, cuDNN, and other relevant libraries, as they often include performance improvements and bug fixes.\\n\\n12. **Hyperparameter Tuning**: Tune hyperparameters related to inference, such as the number of worker threads, to find the optimal configuration for your specific setup.\\n\\nBy following these steps, you should be able to diagnose and address the issue of underutilization of your GPUs. Keep in mind that achieving optimal performance may require iterative testing and tuning of your system's configuration.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =f\"\"\"\n",
    "\n",
    "I have 8GPU of A100, And I am running cuda pipe starting with pipe.to(cuda:0) to up to pipe.to(cuda:7) with \\\n",
    "different ports starign from 9000 to 9007 individually.  \\\n",
    "I used nginx load balancing to send request to this server. \\\n",
    "But When I look into GPU load it some how not correct or it's not using by full potential. \\\n",
    "Can you tell what I need to do here.  \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt=f\"\"\"\n",
    "You are an Expert and Deep understaing deep learning libraies in python. \\\n",
    "Can you solve the error which I am facing while running the model for low latency inference.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc97d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To monitor GPU utilization and send a request to an NGINX server if the utilization is below a certain threshold, you can use the `nvidia-smi` tool (which is part of the NVIDIA driver package) to get GPU utilization information, and the `requests` library in Python to send an HTTP request to the NGINX server.\\n\\nHere's a step-by-step guide on how to implement this in Python:\\n\\n1. Install the `requests` library if you haven't already:\\n\\n```bash\\npip install requests\\n```\\n\\n2. Use the `subprocess` module to run `nvidia-smi` and capture its output.\\n\\n3. Parse the output to get the GPU utilization for each GPU.\\n\\n4. Check if the utilization is below the threshold.\\n\\n5. If it is, send a request to the NGINX server.\\n\\nHere's a sample Python script that demonstrates this process:\\n\\n```python\\nimport subprocess\\nimport re\\nimport requests\\n\\n# Define the threshold for GPU utilization (in percentage)\\nGPU_UTIL_THRESHOLD = 50\\n# URL of the NGINX server to send the request to\\nNGINX_URL = 'http://your-nginx-server-url'\\n\\ndef get_gpu_utilization():\\n    # Run the nvidia-smi command to get GPU stats\\n    result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], stdout=subprocess.PIPE)\\n    # Decode result to string and split by newlines to get each GPU's utilization\\n    gpu_utils = result.stdout.decode('utf-8').strip().split('\\\\n')\\n    # Convert each utilization value to an integer\\n    gpu_utils = [int(x) for x in gpu_utils]\\n    return gpu_utils\\n\\ndef send_request_to_nginx():\\n    # Send a GET request to the NGINX server\\n    response = requests.get(NGINX_URL)\\n    if response.status_code == 200:\\n        print('Request sent successfully to NGINX.')\\n    else:\\n        print('Failed to send request to NGINX. Status code:', response.status_code)\\n\\ndef main():\\n    gpu_utils = get_gpu_utilization()\\n    for gpu_id, gpu_util in enumerate(gpu_utils):\\n        print(f'GPU {gpu_id} Utilization: {gpu_util}%')\\n        if gpu_util < GPU_UTIL_THRESHOLD:\\n            print(f'GPU {gpu_id} is below the threshold. Sending request to NGINX.')\\n            send_request_to_nginx()\\n\\nif __name__ == '__main__':\\n    main()\\n```\\n\\nPlease replace `'http://your-nginx-server-url'` with the actual URL of your NGINX server.\\n\\nThis script will check the utilization of each GPU and if any GPU's utilization is below the defined threshold, it will send a GET request to the specified NGINX server. You can modify the `send_request_to_nginx` function to send a POST request or include additional data as needed for your application.\\n\\nKeep in mind that this script assumes you have NVIDIA GPUs and the `nvidia-smi` tool installed on your system. If you're using a different type of GPU, you'll need to find the appropriate tool or library for querying GPU utilization.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text =f\"\"\"\n",
    "\n",
    "I think I am not calculation Gpu utilisation of all gpus correcly \\\n",
    "I want to calculate gpu utilisation of all gpus \\\n",
    "if that is below some threshold, I will send request to them using NGINX. \\\n",
    "I think nginx will do load balancing accrodingly.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt=f\"\"\"\n",
    "You are an Expert and Deep understaing deep learning libraies in python. \\\n",
    "Can you Implement python code to test GPU Utilisation and send requset \n",
    "\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d40e990c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To understand how GPU generation works when you request for generation in the context of deep learning libraries in Python, let's break down the process:\\n\\n1. **Client Request**: A client (e.g., a web browser or another application) sends a request to a server to perform some computation that requires deep learning models, such as image recognition, natural language processing, etc.\\n\\n2. **Load Balancer (NGINX)**: NGINX, a popular open-source web server, can also act as a reverse proxy and load balancer. It receives the incoming request and decides to which backend server to forward the request based on various load balancing algorithms (e.g., round-robin, least connections, IP-hash, etc.).\\n\\n3. **Server Utilization Check**: Before NGINX forwards the request, it can be configured to check the current load on the backend servers. This can be done using custom health checks or by integrating with a more sophisticated load balancing solution that monitors server utilization.\\n\\n4. **Request Forwarding**: If the server utilization is low (i.e., the server has enough resources to handle additional requests), NGINX forwards the request to the appropriate backend server. If the server is under high load, NGINX can queue the request or forward it to another server with lower utilization.\\n\\n5. **Backend Server Processing**: The backend server, equipped with a GPU, receives the request and starts the computation. The deep learning library (e.g., TensorFlow, PyTorch) utilizes the GPU for accelerating the mathematical operations required by the deep learning models.\\n\\n6. **GPU Acceleration**: The GPU accelerates the computation by parallel processing the tasks. Deep learning libraries are designed to take advantage of the GPU's architecture, which is well-suited for the high volume of matrix and vector operations that are common in deep learning.\\n\\n7. **Response**: Once the computation is complete, the backend server sends the result back to NGINX, which then forwards it to the client.\\n\\nTo set up NGINX for load balancing with server utilization checks, you would typically need to integrate it with additional tools or scripts that can provide real-time metrics about the backend servers' load. Here's a high-level overview of how you might configure NGINX for this purpose:\\n\\n1. **Install NGINX**: Install and configure NGINX on a server that will act as the load balancer.\\n\\n2. **Configure Upstream Servers**: Define the backend servers in the NGINX configuration file (`nginx.conf`) using the `upstream` directive.\\n\\n3. **Load Balancing Method**: Choose a load balancing method and configure it in the `upstream` block.\\n\\n4. **Health Checks**: Implement health checks using NGINX Plus or integrate with a third-party tool that can provide health check functionality.\\n\\n5. **Server Utilization Monitoring**: Set up a system to monitor server utilization, such as using Prometheus with Grafana, or a custom script that can report server load to NGINX.\\n\\n6. **Dynamic Load Balancing**: Use the collected server utilization data to make dynamic load balancing decisions. This might involve custom scripting or using advanced features of NGINX Plus.\\n\\n7. **SSL/TLS Termination**: Optionally, configure NGINX to handle SSL/TLS termination to offload encryption tasks from the backend servers.\\n\\n8. **Logging and Monitoring**: Set up logging and monitoring for NGINX to keep track of its performance and the requests being handled.\\n\\nRemember that this is a simplified overview, and actual implementation can be complex, depending on the specific requirements and infrastructure. Additionally, you may need to consider other factors such as security, failover mechanisms, and session persistence when configuring your load balancing solution.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text =f\"\"\"\n",
    "\n",
    "How the gpu generation works if I request for generation and how to load balance request with NGINX and send the request if server utilisation is low\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt=f\"\"\"\n",
    "You are an Expert and Deep understaing deep learning libraies in python. \\\n",
    "\n",
    "\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2e45b6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string expression part cannot include a backslash (2198776770.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 40\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string expression part cannot include a backslash\n"
     ]
    }
   ],
   "source": [
    "text =f\"\"\"\n",
    " \n",
    " location / {\n",
    "\n",
    "        allow beta.clutchplay.ai;\n",
    "        deny all;\n",
    "\n",
    "      if ($http_referer ~* ^https?://beta\\.clutchplay\\.ai) {\n",
    "            allow all;\n",
    "      }\n",
    "\n",
    "\n",
    "         if ($block_0_0_0) {\n",
    "                return 403;\n",
    "         }\n",
    "         if ($request_method = 'OPTIONS') {\n",
    "            add_header 'Access-Control-Allow-Methods' '*';\n",
    "            add_header 'Access-Control-Allow-Origin' '*';\n",
    "            add_header 'Access-Control-Max-Age' 1728000;\n",
    "            add_header 'Content-Type' 'text/plain charset=UTF-8';\n",
    "            add_header 'Content-Length' 0;\n",
    "            add_header 'Access-Control-Allow-Headers' '*';\n",
    "           add_header 'network_id' '*';\n",
    "         add_header 'network_name' '*';\n",
    "            return 204;\n",
    "        }\n",
    "         add_header 'Access-Control-Allow-Origin' \"*\" always;\n",
    "         add_header 'Access-Control-Expose-Headers' 'Content-Length';\n",
    "         add_header 'Access-Control-Allow-Methods' '*';\n",
    "         add_header 'Access-Control-Allow-Headers' '*';\n",
    "         proxy_set_header 'network_id' $http_network_id;\n",
    "         proxy_set_header 'network_name' $http_network_name;\n",
    "\n",
    "        proxy_pass http://api;\n",
    "        proxy_redirect off;\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "\"\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt=f\"\"\"\n",
    "You are an expert in domain and nginx config \\\n",
    "please help me with adding nginx config fix  \\\n",
    " nginx: [emerg] invalid parameter \"beta.clutchplay.ai\" in /etc/nginx/sites-enabled/beta.api.clutchplay.ai:17 \\\n",
    " \n",
    " with given file\n",
    "\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9941685b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To restrict access to your backend API so that only requests coming from your frontend server at `beta.clutchplay.ai` are allowed, you can use the `allow` and `deny` directives in your Nginx configuration. This can be done by checking the `Referer` header, which indicates the address of the webpage that linked to the resource being requested.\\n\\nHowever, it's important to note that the `Referer` header can be easily spoofed, so this method is not foolproof for security. For stronger security, you should consider implementing authentication tokens, API keys, or other methods of verifying that the request is coming from a trusted source.\\n\\nHere's an example of how you might configure your Nginx to allow requests only from your frontend domain using the `Referer` header:\\n\\n```nginx\\nserver {\\n    listen 80;\\n    server_name beta.api.clutchplay.io;\\n\\n    location / {\\n        # Deny all requests by default\\n        deny all;\\n\\n        # Allow requests with 'Referer' header from your frontend domain\\n        if ($http_referer ~* ^https?://beta\\\\.clutchplay\\\\.ai) {\\n            allow all;\\n        }\\n\\n        # Your proxy or fastcgi configuration goes here\\n        # For example, if you're using proxy_pass to pass requests to a backend server:\\n        # proxy_pass http://backend_upstream;\\n\\n        # Other necessary configuration...\\n    }\\n\\n    # Other server configuration...\\n}\\n```\\n\\nPlease replace `http://backend_upstream` with the actual address of your backend server.\\n\\nKeep in mind that this configuration is based on the `Referer` header, which is not a secure method of protection. For a more secure approach, you should consider the following:\\n\\n1. **API Keys**: Implement API key validation on your backend. Only requests with a valid API key included in the headers or query parameters would be processed.\\n\\n2. **OAuth or JWT**: Use OAuth tokens or JSON Web Tokens (JWT) for more robust authentication and authorization.\\n\\n3. **Rate Limiting**: Implement rate limiting to prevent abuse of your API.\\n\\n4. **Web Application Firewall (WAF)**: Use a WAF to help protect against common web exploits that could affect application availability, compromise security, or consume excessive resources.\\n\\n5. **DDoS Protection Services**: Consider using services like Cloudflare or AWS Shield for advanced DDoS mitigation.\\n\\nRemember to reload or restart your Nginx server after making changes to the configuration:\\n\\n```bash\\nsudo nginx -t      # Test the configuration for syntax errors\\nsudo systemctl reload nginx  # Reload the configuration without downtime\\n# or\\nsudo systemctl restart nginx # Restart the Nginx service\\n```\\n\\nAlways test your configuration changes in a safe environment before applying them to your production server.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text =f\"\"\"\n",
    "I have nginx config which contains beta.api.clutchplay.io for backend server hosted \\\n",
    "beta.clutchplay.ai for frontend hosted, but for backend server i am continsoly getting ddos \\\n",
    "requests, so I am lookin to add allow from beta.clutchplay.ai to beta.api.clutchplay.io so users \\\n",
    "unable to accesss backend apis from other services like postman\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "prompt=f\"\"\"\n",
    "You are an expert in domain and nginx config \\\n",
    "please help me with adding nginx config \n",
    "\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe6476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
